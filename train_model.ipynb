{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce39f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "dataset_dir = \"./dataset\"\n",
    "exist_dataset_dir =  os.path.exists(dataset_dir)\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jhonromero26/voice-commands-spanish\")\n",
    "\n",
    "if not exist_dataset_dir or path:\n",
    "    !rm -rf {dataset_dir}\n",
    "    !cp -r {path}/dataset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5be171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Download MIR RIR data\n",
    "ouput_folder = \"./noising_dataset\"\n",
    "output_dir = f\"{ouput_folder}/mit_rirs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    for row in tqdm(rir_dataset):\n",
    "        name = row['audio'].metadata.path.split('/')[-1]\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "## Download noise and background audio\n",
    "\n",
    "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
    "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
    "# For full-scale training, it's recommended to download the entire dataset from\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
    "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
    "\n",
    "output_dir = f\"{ouput_folder}/audioset\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    fname = \"bal_train09.tar\"\n",
    "    out_dir = f\"{output_dir}/{fname}\"\n",
    "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
    "    !wget -O {out_dir} {link}\n",
    "    !cd {output_dir} && tar -xf {fname}\n",
    "    !rm -rf {fname}\n",
    "\n",
    "    subout_dir = f\"{ouput_folder}/audioset_16k\"\n",
    "    if not os.path.exists(subout_dir):\n",
    "        os.makedirs(subout_dir, exist_ok=True)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(f\"{output_dir}/audio\").glob(\"**/*.flac\")]})\n",
    "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(audioset_dataset):\n",
    "        name = row['audio'].metadata.path.split('/')[-1].replace(\".flac\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(subout_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "# (Third-party mchl914 extra small set)\n",
    "\n",
    "output_dir = f\"{ouput_folder}/fma\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    fname = \"fma_xs.zip\"\n",
    "    link = \"https://huggingface.co/datasets/mchl914/fma_xsmall/resolve/main/\" + fname\n",
    "    out_dir = f\"{output_dir}/{fname}\"\n",
    "    !wget -O {out_dir} {link}\n",
    "    !cd {output_dir} && unzip -q {fname}\n",
    "    !rm -rf {fname}\n",
    "\n",
    "    subout_dir = f\"{ouput_folder}/fma_16k\"\n",
    "    if not os.path.exists(subout_dir):\n",
    "        os.mkdir(subout_dir)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    fma_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(f\"{output_dir}/fma_small\").glob(\"**/*.mp3\")]})\n",
    "    fma_dataset = fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(fma_dataset):\n",
    "        name = row['audio'].metadata.path.split('/')[-1].replace(\".mp3\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(subout_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the augmentations.\n",
    "# To improve your model, experiment with these settings and use more sources of\n",
    "# background clips.\n",
    "\n",
    "import os\n",
    "from speech_commands.audio.augmentation import Augmentation\n",
    "from speech_commands.audio.clips import Clips\n",
    "\n",
    "\n",
    "commands_directory = './dataset'\n",
    "commands_dirs = os.listdir(commands_directory)\n",
    "\n",
    "clips = {}\n",
    "for folder in commands_dirs:\n",
    "    folder_path = f'{commands_directory}/{folder}'\n",
    "    clips[folder] = Clips(input_directory=folder_path,\n",
    "            file_pattern='*.wav',\n",
    "            max_clip_duration_s=None,\n",
    "            remove_silence=False,\n",
    "            random_split_seed=10,\n",
    "            split_count=0.1,\n",
    "            )\n",
    "\n",
    "\n",
    "augmenter = Augmentation(augmentation_duration_s=3.2,\n",
    "                         augmentation_probabilities = {\n",
    "                                \"SevenBandParametricEQ\": 0.1,\n",
    "                                \"TanhDistortion\": 0.1,\n",
    "                                \"PitchShift\": 0.1,\n",
    "                                \"BandStopFilter\": 0.1,\n",
    "                                \"AddColorNoise\": 0.1,\n",
    "                                \"AddBackgroundNoise\": 0.75,\n",
    "                                \"Gain\": 1.0,\n",
    "                                \"RIR\": 0.5,\n",
    "                            },\n",
    "                         impulse_paths = ['./noising_dataset/mit_rirs'],\n",
    "                         background_paths = ['./noising_dataset/fma_16k', './noising_dataset/audioset_16k'],\n",
    "                         background_min_snr_db = -5,\n",
    "                         background_max_snr_db = 10,\n",
    "                         min_jitter_s = 0.195,\n",
    "                         max_jitter_s = 0.205,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment a random clip and play it back to verify it works well\n",
    "\n",
    "from IPython.display import Audio\n",
    "from speech_commands.audio.audio_utils import save_clip\n",
    "    \n",
    "random_clip = clips['adelante'].get_random_clip()\n",
    "augmented_clip = augmenter.augment_clip(random_clip)\n",
    "save_clip(augmented_clip, 'augmented_clip.wav')\n",
    "\n",
    "display(Audio(\"augmented_clip.wav\", autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a03f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment samples and save the training, validation, and testing sets.\n",
    "# Validating and testing samples generated the same way can make the model\n",
    "# benchmark better than it performs in real-word use. Use real samples or TTS\n",
    "# samples generated with a different TTS engine to potentially get more accurate\n",
    "# benchmarks.\n",
    "\n",
    "import os\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "from speech_commands.audio.spectrograms import SpectrogramGeneration\n",
    "\n",
    "\n",
    "output_dataset = 'commands_augmented'\n",
    "commands_dir = f'{output_dataset}/commands'\n",
    "negative_dir = f'{output_dataset}/negative'\n",
    "splits = [\"training\", \"validation\", \"testing\"]\n",
    "\n",
    "\n",
    "if not os.path.exists(commands_dir):\n",
    "    os.makedirs(commands_dir)\n",
    "\n",
    "\n",
    "for cmd in clips.keys():\n",
    "  nmap_name = 'command_mmap'\n",
    "  output_dir = os.path.join(commands_dir)\n",
    "\n",
    "  for split in splits:\n",
    "    out_dir = os.path.join(output_dir, cmd, split)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "    split_name = \"train\"\n",
    "    repetition = 2\n",
    "    \n",
    "  \n",
    "    spectrograms = SpectrogramGeneration(clips=clips[cmd],\n",
    "                                      augmenter=augmenter,\n",
    "                                      slide_frames=10,    # Uses the same spectrogram repeatedly, just shifted over by one frame. This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
    "                                      step_ms=10,\n",
    "                                      )\n",
    "    \n",
    "    if split == \"validation\":\n",
    "      split_name = \"validation\"\n",
    "      repetition = 1\n",
    "    elif split == \"testing\":\n",
    "      split_name = \"test\"\n",
    "      repetition = 1\n",
    "      spectrograms = SpectrogramGeneration(clips=clips[cmd],\n",
    "                                      augmenter=augmenter,\n",
    "                                      slide_frames=1,    # The testing set uses the streaming version of the model, so no artificial repetition is necessary\n",
    "                                      step_ms=10,\n",
    "                                      )\n",
    "\n",
    "    RaggedMmap.from_generator(\n",
    "        out_dir=os.path.join(out_dir, 'command_mmap'),\n",
    "        sample_generator=spectrograms.spectrogram_generator(split=split_name, repeat=repetition),\n",
    "        batch_size=100,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads pre-generated spectrogram features (made for microWakeWord in\n",
    "# particular) for various negative datasets. This can be slow!\n",
    "\n",
    "output_dir = f'{output_dataset}/negative'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
    "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
    "    for fname in filenames:\n",
    "        link = link_root + fname\n",
    "\n",
    "        zip_path = f\"{output_dir}/{fname}\"\n",
    "        !wget -O {zip_path} {link}\n",
    "        !unzip -q {zip_path} -d {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404d54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a yaml config that controls the training process\n",
    "# These hyperparamters can make a huge different in model quality.\n",
    "# Experiment with sampling and penalty weights and increasing the number of\n",
    "# training steps.\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "output_dataset = 'commands_augmented'\n",
    "negative_dir = f'{output_dataset}/negative'\n",
    "commands_dir = f'{output_dataset}/commands'\n",
    "\n",
    "voice_commands = os.listdir(commands_dir)\n",
    "total_commands = len(voice_commands)\n",
    "num_classes = total_commands + 1\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"clip_duration_ms\": 1500,\n",
    "    \"training_steps\": [20000],\n",
    "    \"learning_rates\": [0.001],\n",
    "    \"window_step_ms\": 10,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"train_dir\": \"trained\",\n",
    "}\n",
    "\n",
    "config['features'] = [\n",
    "    {\n",
    "        \"features_dir\": f\"{negative_dir}/speech\",\n",
    "        \"label\": 0,\n",
    "        \"truth\": False,\n",
    "        \"sampling_weight\": 10.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truncation_strategy\": \"truncate_end\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": f\"{negative_dir}/dinner_party\",\n",
    "        \"label\": 0,\n",
    "        \"truth\": False,\n",
    "        \"sampling_weight\": 10.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truncation_strategy\": \"truncate_end\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": f\"{negative_dir}/no_speech\",\n",
    "        \"label\": 0,\n",
    "        \"truth\": False,\n",
    "        \"sampling_weight\": 5.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truncation_strategy\": \"truncate_end\",\n",
    "        \"type\": \"mmap\",\n",
    "    }\n",
    "]\n",
    "\n",
    "labels = [(\"negative\", 0)]\n",
    "for i in range(total_commands):\n",
    "    labels.append((voice_commands[i], i + 1))\n",
    "    config['features'].append(\n",
    "        {\n",
    "            \"features_dir\": f\"{commands_dir}/{voice_commands[i]}\",\n",
    "            \"label\": i + 1,\n",
    "            \"truth\": True,\n",
    "            \"sampling_weight\": 1.0,\n",
    "            \"penalty_weight\": 1.0,\n",
    "            \"truncation_strategy\": \"truncate_start\",\n",
    "            \"type\": \"mmap\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Number of training steps in each iteration - various other settings are configured as lists that corresponds to different steps\n",
    "config[\"training_steps\"] = [20000]\n",
    "\n",
    "# Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
    "config[\"positive_class_weight\"] = [1]\n",
    "config[\"negative_class_weight\"] = [20]\n",
    "\n",
    "config[\"learning_rates\"] = [\n",
    "    0.001,\n",
    "]  # Learning rates for Adam optimizer - list that corresponds to training steps\n",
    "config[\"batch_size\"] = 128\n",
    "\n",
    "config[\"time_mask_max_size\"] = [\n",
    "    0\n",
    "]  # SpecAugment - list that corresponds to training steps\n",
    "config[\"time_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
    "config[\"freq_mask_max_size\"] = [\n",
    "    0\n",
    "]  # SpecAugment - list that corresponds to training steps\n",
    "config[\"freq_mask_count\"] = [0]  # SpecAugment - list that corresponds to training steps\n",
    "\n",
    "config[\"eval_step_interval\"] = (\n",
    "    500  # Test the validation sets after every this many steps\n",
    ")\n",
    "config[\"clip_duration_ms\"] = (\n",
    "    1500  # Maximum length of wake word that the streaming model will accept\n",
    ")\n",
    "\n",
    "# The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
    "# Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
    "# Available metrics:\n",
    "#   - \"loss\" - cross entropy error on validation set\n",
    "#   - \"accuracy\" - accuracy of validation set\n",
    "#   - \"recall\" - recall of validation set\n",
    "#   - \"precision\" - precision of validation set\n",
    "#   - \"false_positive_rate\" - false positive rate of validation set\n",
    "#   - \"false_negative_rate\" - false negative rate of validation set\n",
    "#   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
    "#   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
    "config[\"target_minimization\"] = 0.9\n",
    "config[\"minimization_metric\"] = None  # Set to None to disable\n",
    "\n",
    "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
    "\n",
    "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
    "    documents = yaml.dump(config, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008a62f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-08 08:24:06.397611: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/romero/Downloads/microwakemodel/main.py\", line 34, in <module>\n",
      "    import microwakeword.data as input_data\n",
      "ModuleNotFoundError: No module named 'microwakeword'\n"
     ]
    }
   ],
   "source": [
    "# Trains a model. When finished, it will quantize and convert the model to a\n",
    "# streaming version suitable for on-device detection.\n",
    "# It will resume if stopped, but it will start over at the configured training\n",
    "# steps in the yaml file.\n",
    "# Change --train 0 to only convert and test the best-weighted model.\n",
    "# On Google colab, it doesn't print the mini-batch results, so it may appear\n",
    "# stuck for several minutes! Additionally, it is very slow compared to training\n",
    "# on a local GPU.\n",
    "\n",
    "!python main.py \\\n",
    "--training_config='training_parameters.yaml' \\\n",
    "--train 1 \\\n",
    "--restore_checkpoint 1 \\\n",
    "--test_tf_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming_quantized 1 \\\n",
    "--use_weights \"best_weights\" \\\n",
    "mixednet \\\n",
    "--pointwise_filters \"64,64,64,64\" \\\n",
    "--repeat_in_block  \"1, 1, 1, 1\" \\\n",
    "--mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
    "--residual_connection \"0,0,0,0\" \\\n",
    "--first_conv_filters 32 \\\n",
    "--first_conv_kernel_size 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microwakemodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
